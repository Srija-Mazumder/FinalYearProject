{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file at: normalized_data.csv\n",
      "File found: normalized_data.csv\n",
      "Data successfully loaded!\n",
      "PCA applied successfully!\n",
      "PCA Data Shape: (54675, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "file_path = r'normalized_data.csv'\n",
    "\n",
    "# Step 2: Check if the file exists\n",
    "print(f\"Checking file at: {file_path}\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    print(f\"File found: {file_path}\")\n",
    "\n",
    "    # Step 3: Load the data\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Data successfully loaded!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Step 5: Apply PCA\n",
    "    try:\n",
    "        pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "        pca_data = pca.fit_transform(data.iloc[:, 1:])  # Assuming first column is non-numerical\n",
    "        print(\"PCA applied successfully!\")\n",
    "        print(f\"PCA Data Shape: {pca_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Step 6: Perform DBSCAN clustering\n",
    "    try:\n",
    "        # Debugging: Check for NaN or Inf values\n",
    "        if pd.DataFrame(pca_data).isnull().values.any():\n",
    "            print(\"PCA Data contains NaN values. Please check your input.\")\n",
    "            exit()\n",
    "\n",
    "        # Adjust DBSCAN parameters as needed\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)  # Modify eps and min_samples for your dataset\n",
    "        labels = dbscan.fit_predict(pca_data)\n",
    "\n",
    "        # Debugging: Output cluster labels\n",
    "        print(f\"DBSCAN Labels: {labels}\")\n",
    "\n",
    "        # Add the cluster labels to the dataset\n",
    "        data['Cluster'] = labels\n",
    "        print(\"DBSCAN clustering completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during clustering: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Step 7: Find the approximate cluster centers\n",
    "    cluster_centers = {}\n",
    "    try:\n",
    "        for cluster_label in np.unique(labels):\n",
    "            if cluster_label != -1:  # Ignore noise points (-1)\n",
    "                cluster_points = pca_data[labels == cluster_label]\n",
    "                center = np.mean(cluster_points, axis=0)\n",
    "                cluster_centers[cluster_label] = center\n",
    "\n",
    "        # Debugging: Output the cluster centers\n",
    "        print(f\"Cluster centers: {cluster_centers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cluster center calculation: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Step 8: Save rows corresponding to each cluster to a CSV file\n",
    "    try:\n",
    "        # Create separate DataFrames for core points, border points, and noise points\n",
    "        core_points = pd.DataFrame()\n",
    "        border_points = pd.DataFrame()\n",
    "        noise_points = pd.DataFrame()\n",
    "\n",
    "        # Get the core points\n",
    "        core_mask = (labels != -1)  # Exclude noise points\n",
    "        for i, label in enumerate(labels):\n",
    "            if label != -1 and np.sum(labels == label) >= dbscan.min_samples:\n",
    "                core_points = pd.concat([core_points, data.iloc[i:i+1]], ignore_index=True)\n",
    "\n",
    "        # Get the border points (those within epsilon of core points but not core points themselves)\n",
    "        for i, label in enumerate(labels):\n",
    "            if label != -1 and np.sum(labels == label) < dbscan.min_samples:\n",
    "                border_points = pd.concat([border_points, data.iloc[i:i+1]], ignore_index=True)\n",
    "\n",
    "        # Get the noise points (those labeled as -1)\n",
    "        noise_points = data[labels == -1]\n",
    "\n",
    "        # Save the data to separate CSV files\n",
    "        output_dir = r'C:\\Users\\Mili\\Desktop\\Project'  # Change path if necessary\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Save each category of points to CSV files\n",
    "        core_file = os.path.join(output_dir, 'core_points.csv')\n",
    "        border_file = os.path.join(output_dir, 'border_points.csv')\n",
    "        noise_file = os.path.join(output_dir, 'noise_points.csv')\n",
    "\n",
    "        core_points.to_csv(core_file, index=False)\n",
    "        border_points.to_csv(border_file, index=False)\n",
    "        noise_points.to_csv(noise_file, index=False)\n",
    "\n",
    "        print(f\"Core points saved to: {core_file}\")\n",
    "        print(f\"Border points saved to: {border_file}\")\n",
    "        print(f\"Noise points saved to: {noise_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving clustered data with centers: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Step 9: Plot DBSCAN results\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot clusters using the PCA-reduced data\n",
    "        plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis', marker='o', s=50, alpha=0.6)\n",
    "\n",
    "        # Highlight noise points (label = -1)\n",
    "        noise_points = (labels == -1)\n",
    "        plt.scatter(pca_data[noise_points, 0], pca_data[noise_points, 1], c='red', marker='x', label='Noise', s=100)\n",
    "\n",
    "        # Plot cluster centers\n",
    "        for center in cluster_centers.values():\n",
    "            plt.scatter(center[0], center[1], c='black', marker='x', s=200, edgecolors='k', label='Cluster Center')\n",
    "\n",
    "        plt.title('DBSCAN Clustering Results with PCA')\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during plotting: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Mili\\Desktop\\Project\n",
      "Files in the current directory: ['Main.ipynb', 'normalized_data.csv.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Files in the current directory: {os.listdir()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
